project: "Fetch_MultiTask"
total_timesteps: 10_000_000
checkpoints: 2_000
eval_freq: 2_000
record_freq: -1
# record_freq: 100_000

env:
  name: "FetchPickAndPlace-v2"
  args:
    reward_type: "sparse"
    # tasks_to_complete:
    #   - "microwave"

    # terminate_on_tasks_completed: True
    # remove_task_when_completed: True
    # object_noise_ratio: 0.0005
    # robot_noise_ratio: 0.01
    max_episode_steps: 100
  wraps:
    - wrap:
        name: "reward_wrap"
        module: "wraps.reward"
        args:
          reward_scale: 1.0
    - wrap:
        name: "goal_conditioned_wrap"
        module: "wraps.observation"


policy: 
  name: "SACPolicy"
  module: "stable_baselines3.sac.policies"

algorithm: 
  name: "SAC"
  module: "agents.mb_sac"
  args:
      learning_rate: 0.0003
      buffer_size: 100_000
      learning_starts: 256
      batch_size: 64
      tau: 0.005
      gamma: 0.99
      K: 64
      train_freq: 1
      gradient_steps: 1
      lambda_cai: 0.2
      cai_clip: Null
      world_steps_to_train: 50 #-1 to not train MAYBE make more frequent updates
      world_num_updates: 20
      world_batch_size: 1024
      ent_coef: auto # Number for fixed, "auto" for learned
      action_noise: Null
      seed: Null
      device: "cuda:1"
      model_path: Null
      world_model:
        name: "mlp"
        module: "models.world_model"
        pretrained: False
        path: Null
        args:
          hidden_dims: [4048, 2012, 512, 206]
          hidden_activation: "ReLU"
      policy_kwargs:
        net_arch: [512, 256, 256]

