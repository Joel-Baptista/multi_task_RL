project: "Fetch_MultiTask"
total_timesteps: 10_000_000
checkpoints: 2_000
eval_freq: 2_000
record_freq: -1
# record_freq: 100_000

env:
  name: "FetchPickAndPlace-v2"
  args:
    reward_type: "dense"
    # tasks_to_complete:
    #   - "microwave"

    # terminate_on_tasks_completed: True
    # remove_task_when_completed: True
    # object_noise_ratio: 0.0005
    # robot_noise_ratio: 0.01
    max_episode_steps: 50
  wraps:
    - wrap:
        name: "tool_distance"
        module: "wraps.reward"
        args:
          reward_scale: 1.0
          tool_weight: 0.5
    - wrap:
        name: "goal_conditioned_wrap"
        module: "wraps.observation"

policy: 
  name: "MlpPolicy"
  module: "stable_baselines3.ppo.policies"

algorithm: 
  name: "PPO"
  module: "stable_baselines3"
  args:
    learning_rate: 0.00003
    n_steps: 4096
    batch_size: 64
    n_epochs: 18
    gamma: 0.87
    gae_lambda: 0.99
    clip_range: 0.1
    clip_range_vf: Null
    normalize_advantage: True
    ent_coef: 0.001
    vf_coef: 0.5
    max_grad_norm: 0.5
    use_sde: False
    sde_sample_freq: -1
    target_kl: 0.27
    seed: Null
    stats_window_size: 1_000
    device: "cuda:0"
    policy_kwargs:
      net_arch: [512, 512, 256, 256]
      log_std_init: -1

