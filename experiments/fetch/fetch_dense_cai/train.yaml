project: "Fetch_MultiTask"
total_timesteps: 10_000_000
checkpoints: 2_000
eval_freq: 2_000
record_freq: -1
# record_freq: 100_000

env:
  name: "FetchPickAndPlace-v2"
  args:
    reward_type: "dense"
    max_episode_steps: 50
  wraps:
    - wrap:
        name: "reward_wrap"
        module: "wraps.reward"
        args:
          reward_scale: 1.0
    - wrap:
        name: "goal_conditioned_wrap"
        module: "wraps.observation"


policy: 
  name: "SACPolicy"
  module: "stable_baselines3.sac.policies"

algorithm: 
  name: "SAC"
  module: "agents.mb_sac"
  args:
      learning_rate: 0.0003
      buffer_size: 1_000_000
      learning_starts: 256
      batch_size: 64
      tau: 0.005
      gamma: 0.99
      K: 64
      train_freq: 1
      gradient_steps: 1
      lambda_cai: 0.2
      cai_clip: Null
      world_steps_to_train: -1 #-1 to not train 
      world_num_updates: 20
      world_batch_size: 1024
      ent_coef: auto # Number for fixed, "auto" for learned
      action_noise: Null
      seed: Null
      device: "cuda:1"
      model_path: Null
      stats_window_size: 1_000
      world_model:
        name: "mlp"
        module: "models.world_model"
        pretrained: True
        path: "fetch_world_model"
        partial_obs: [0, 25] #can be Null for full observation
        args:
          hidden_dims: [4096, 4096, 4096, 4096, 4096, 2048, 1024, 512]
          hidden_activation: "ReLU"
      policy_kwargs:
        net_arch: [512, 256, 256]

