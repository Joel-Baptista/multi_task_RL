Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
ActorCriticPolicy(
  (features_extractor): FlattenExtractor(
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
  (pi_features_extractor): FlattenExtractor(
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
  (vf_features_extractor): FlattenExtractor(
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
  (mlp_extractor): MlpExtractor(
    (policy_net): Sequential(
      (0): Linear(in_features=61, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=64, bias=True)
      (3): Tanh()
    )
    (value_net): Sequential(
      (0): Linear(in_features=61, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=64, bias=True)
      (3): Tanh()
    )
  )
  (action_net): Linear(in_features=64, out_features=9, bias=True)
  (value_net): Linear(in_features=64, out_features=1, bias=True)
)
Logging to /home/joel/PhD/results/models/baseline/ch22p83h/PPO_1
[34m[1mwandb[39m[22m: [33mWARNING[39m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /home/joel/PhD/results/models/baseline/ch22p83h/PPO_1
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 280      |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 326      |
|    iterations      | 1        |
|    time_elapsed    | 6        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 280         |
|    ep_rew_mean          | 0           |
| time/                   |             |
|    fps                  | 308         |
|    iterations           | 2           |
|    time_elapsed         | 13          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.010500951 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -12.8       |
|    explained_variance   | -1.39       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0119     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0146     |
|    std                  | 1           |
|    value_loss           | 0.00413     |
-----------------------------------------
Traceback (most recent call last):
  File "/home/joel/PhD/ws/multi_task_RL/train_gym_robot.py", line 145, in <module>
    main()
  File "/home/joel/PhD/ws/multi_task_RL/train_gym_robot.py", line 130, in main
    model.learn(
  File "/home/joel/PhD/.envs/gym-torch/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py", line 308, in learn
    return super().learn(
           ^^^^^^^^^^^^^^
  File "/home/joel/PhD/.envs/gym-torch/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 259, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/joel/PhD/.envs/gym-torch/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 169, in collect_rollouts
    actions, values, log_probs = self.policy(obs_tensor)
                                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/joel/PhD/.envs/gym-torch/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/joel/PhD/.envs/gym-torch/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/joel/PhD/.envs/gym-torch/lib/python3.11/site-packages/stable_baselines3/common/policies.py", line 628, in forward
    log_prob = distribution.log_prob(actions)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/joel/PhD/.envs/gym-torch/lib/python3.11/site-packages/stable_baselines3/common/distributions.py", line 175, in log_prob
    log_prob = self.distribution.log_prob(actions)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/joel/PhD/.envs/gym-torch/lib/python3.11/site-packages/torch/distributions/normal.py", line 83, in log_prob
    math.log(self.scale) if isinstance(self.scale, Real) else self.scale.log()
                                                              ^^^^^^^^^^^^^^^^
KeyboardInterrupt